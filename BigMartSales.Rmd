# Big Mart Sales Prediction 

##Introduction 
### Problem Statement
The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.

Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.

 

Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.

### Data sets

We have train (8523) and test (5681) data set, train data set has both input and output variable(s). The goal is to predict the sales for test data set.

### Summary 

After we loaded the data and the required libaries, we explored the data and its variables and made some observations. Afterwards, we imputed the missing values, and we continued with the feature engineering process. We have built a baseline model and five different other models to fit our data: Linear Regression, Ridge and Lasso Regression, Decision Tree, Random Forest and EXtreme Gradient Boost. 
We divided the training set to secondary training and test set, to evaluate the Root Mean Squared Error (RMSE) between the prediction and the actual values on them before training the whole training set and predict values for the actual given test set. The Root Mean Squared Error (RMSE) was used as the evaluation metric in this analysis. We concluded that the model built using the XGBoost algorithm was the most efficient (lowest RMSE), followed by the Random Forest model. At last, we trained the whole training data set using these two models and we predicted the valus of Item_Outlet_Sales, which was the target variable, of the given test set. 



## Load the data and the libraries 

The needed libraries are loaded. 
```{r}
library(caret)
library(dplyr)
library(reshape2)
library(elasticnet)
library(glmnet)
library(rpart)
library(randomForest)
library(ranger)
library(xgboost)
```

The train and test data sets are loaded. 
```{r}
train <- read.csv("TrainSales.csv",header = TRUE, na.strings = "")
test <- read.csv("TestSales.csv", header = TRUE, na.strings = "")
```

## Explore the data 

We combine the train and test data.
```{r}
train2 <- train
test2 <- test
test2$Item_Outlet_Sales <- rep(NA, times = 5681)
comb2 <- rbind(train2,test2)
```

The number of missing values of every variable is calculated.
```{r}
temp <- lapply(comb2, is.na)
sapply(temp, sum)
```

The Item_Outlet_Sales is the target variable for prediction, and as a result the missing values are from the test data set. It can be seen that the Item_Weight and Outlet_Size variables have some missing values, which need imputing. 

```{r}
str(comb2)
```

We summarize and examine the numerical variables of the data set. 
```{r}
num_vars <- c("Item_Weight", "Item_Visibility","Item_MRP","Outlet_Establishment_Year","Item_Outlet_Sales")
summary(comb2[,num_vars])
```

We explore the categorical variables of the data set. We see how many unique values has each categorical variable. We make a frequency table of the variables, excluding the Item_Identifier and Outlet_Identifier variables. 
```{r}
summary(comb2[,!(colnames(comb2) %in% num_vars)])
v<- sapply(comb2[,!(colnames(comb2) %in% num_vars)], unique)
sapply(v, length)
lapply((comb2[,!(colnames(comb2) %in% num_vars | colnames(comb2) %in% c("Item_Identifier", "Outlet_Identifier"))]), table)
```

Observations:   

1. In the Item_Fat_Content variable, some "Low Fat" products have been miscoded as "low fat" or "LF", while some "Regular" products have been miscoded as "reg". 

2. In the Item_Type variable, not all categories have substantial numbers. Combining them may give better results.

## Clean the data 

### Impute the missing values 

There are two variables with missing values: Item_Weight and Outlet_Size.

#### Item_Weight 

The missing values of the Item_Weight variable are going to be imputed with the average weigth for the particular item. 

```{r}
## We find the average weight for each Item. 
AvweightByItem <- aggregate(Item_Weight~Item_Identifier, data = comb2, mean)

## Impute the missing values 
for(i in 1:(dim(comb2)[1]))
{
        if(is.na(comb2$Item_Weight[i])){
                pr <- comb2$Item_Identifier[i]
                comb2$Item_Weight[i] <-  AvweightByItem[pr,2]
        }
}
sum(is.na(comb2$Item_Weight))
```

#### Outlet_Size

The missing values of the Outlet_Size variable will be imputed with the appropriate size respective of the Outlet_Type. 

```{r}
t <- table(comb2$Outlet_Type, comb2$Outlet_Size)

for (i in 1:(dim(comb2)[1])){
        if(is.na(comb2$Outlet_Size[i])){
                if(comb2$Outlet_Type[i] == "Grocery Store"){
                        comb2$Outlet_Size[i] <- "Small"
                }
                else if(comb2$Outlet_Type[i] == "Supermarket Type1"){
                        comb2$Outlet_Size[i] <- sample(c("High","Medium",
                                                         "Small"),size = 1,
                                                       replace = TRUE, prob = c(t["Supermarket Type1",1]/sum(t["Supermarket Type1",]),
                                                                                (t["Supermarket Type1",2]/sum(t["Supermarket Type1",])),
                                                                                (t["Supermarket Type1",3]/sum(t["Supermarket Type1",]))))
                        
                }
                else if(comb2$Item_Type[i] == "Supermarket Type2"|comb2$Item_Type[i] == "Supermarket Type3"){
                        comb2$Outlet_Size[i] <- "Medium"
                }
                        
                }
}

sum(is.na(comb2$Outlet_Size))

table(comb2$Outlet_Size, comb2$Outlet_Type)
```

## Feature Engineering

### Item_Visibility Modify

```{r}
summary(comb2$Item_Visibility)
```
It can be seen that the minimum value of the Item_Visibility variable is 0, which is not logical, as it makes no sense to have an item in a store without displaying it. The zeros in the variable values will be considered as missing values and will be imputed with the mean visibility of the product. 

```{r}
## We find the average visibility for each Item. 
AvVisibilityByItem <- aggregate(Item_Visibility~Item_Identifier, data = comb2, mean)

## Impute the missing values 
for(i in 1:(dim(comb2)[1]))
{
        if(comb2$Item_Visibility[i] == 0){
                pr2 <- comb2$Item_Identifier[i]
                comb2$Item_Visibility[i] <-  AvVisibilityByItem[pr,2]
        }
}
sum(which(comb2$Item_Visibility == 0))

```

A new variable is created which represents the ratio of the mean value of the visibility of an item in a particular store over the mean visibility of the item in general and it is named Item_Visibility_Means_ratio.  

```{r}
comb2$Item_Visibility_Means_ratio <- rep(0, times = dim(comb2)[1])

for(i in 1:(dim(comb2)[1])){
        comb2$Item_Visibility_Means_ratio[i] <- (comb2[i,"Item_Visibility"])/(AvVisibilityByItem[AvVisibilityByItem$Item_Identifier == comb2$Item_Identifier[i],"Item_Visibility"])
}
```
### Create variable Item_Type_General (a new variable with broader Item type categories)

A new varaible is created with the name of Item_Type_General, which will be a broader type of product variable, with the values "Food", "Drinks" and "Non-Concumable", and will be constructed using the first letters of the Item_Identifier variable. 

```{r}
comb2$Item_Type_General <- rep("a", times = dim(comb2)[1])

for(i in 1:(dim(comb2)[1])){
        if(startsWith(as.character(comb2$Item_Identifier[i]),"FD")){
                comb2$Item_Type_General[i] <- "Food"
        }
        else if(startsWith(as.character(comb2$Item_Identifier[i]),"DR")){
                comb2$Item_Type_General[i] <- "Drinks"
        }
        else if(startsWith(as.character(comb2$Item_Identifier[i]),"NC")){
                comb2$Item_Type_General[i] <- "Non-Consumable"
        }
}

```

### Item_Fat_Contents Modify 

Even though there are five values for this variable, it actually has two  categories: "Low Fat" and "Regular". The existance of five values is because of different represantation of the same value. Products that are
"Low Fat" have been categorised ,in a wrong way, as "low fat" and "LF" also and some products that are "Regular" have been categorised mistakenly as "reg". 

Also, a new category should be made as there are some products that are not consumable. The new catgory will be "Non-Edible". 

```{r}
comb2$Item_Fat_Content <- as.character(comb2$Item_Fat_Content)

for(i in 1:(dim(comb2)[1])){
        if(startsWith(as.character(comb2$Item_Identifier[i]),"NC")){
                comb2$Item_Fat_Content[i] <- "Non-Edible"
                
        }
        else{
                if(comb2$Item_Fat_Content[i]=="low fat" | comb2$Item_Fat_Content[i] == "LF"){
                        comb2$Item_Fat_Content[i] <- "Low Fat"
                }
                else if(comb2$Item_Fat_Content[i] == "reg"){
                        comb2$Item_Fat_Content[i] <- "Regular"
                }
        }
}
comb2$Item_Fat_Content <- as.factor(comb2$Item_Fat_Content)
```

### Create variable Years_Operate 

Years_Operate is a new variable which represents the time in yars that a store has benn perated. 

```{r}
comb2$Years_Operate <- 2013 - comb2$Outlet_Establishment_Year
```

### New variale Outlet

New variable Outlet is created with levels 0 - 9,same as the Outlet_Identifier.
```{r}
comb2$Outlet <- comb2$Outlet_Identifier
levels(comb2$Outlet)<- 0:9
summary(comb2$Outlet)

```

### One hot coding of the categorical variables 

One hot coding of the categorical variables in a new data frame. 

```{r}
comb2$Outlet <- comb2$Outlet_Identifier
levels(comb2$Outlet)<- 0:9

comb3 <- comb2
levels(comb3$Outlet_Location_Type) <- 1:3
levels(comb3$Outlet_Type) <- 0:3
dm <- dummyVars(~ Item_Fat_Content + Outlet_Size + Outlet_Location_Type +
                        Outlet_Type+Item_Type_General+ Outlet, data = comb3)
tr <- data.frame(predict(dm, newdata = comb3))

dum <- c("Item_Fat_Content", "Outlet_Size", "Outlet_Location_Type",
                 "Outlet_Type", "Item_Type_General", "Outlet")
comb4 <- cbind(comb3[,!(colnames(comb3) %in% dum)],tr)
comb4 <- comb4[,-c(4,7)]
```

## Seperate the data in training and testing data 

```{r}
train4 <- comb4[!(is.na(comb4$Item_Outlet_Sales)),]
test4 <- comb4[is.na(comb4$Item_Outlet_Sales),]

test4$Item_Outlet_SalesN <- test4$Item_Outlet_Sales
test4 <- test4[,-6]
train4$Item_Outlet_SalesN <- train$Item_Outlet_Sales
train4 <- train4[,-6]
test4 <- test4[,-34]
train4$Item_Outlet_Sales <- train4$Item_Outlet_SalesN
train4<- train4[,-34]
```

## Seperate the training data for cross - validation. 

```{r}
inTrain <- createDataPartition(train4$Item_Outlet_Sales, p = 0.75, list = FALSE)
train4tr <- train4[inTrain,]
train4te <- train4[-inTrain,]
```

## Model Building 

### Baseline model 

A baseline model is made using the mean of  the overall sales. 

```{r}
bmodel_meanS <- mean(train4tr$Item_Outlet_Sales)
bmodel_meanS_pred <- rep(bmodel_meanS, times = dim(train4tr)[1])
rmsetrS <- sqrt(mean((bmodel_meanS_pred - train4tr$Item_Outlet_Sales)^2))
rmsetrS

bmodel_meanS_predte <- rep(bmodel_meanS, times = dim(train4te)[1])
rmsetrSte <- sqrt(mean((bmodel_meanS_predte - train4te$Item_Outlet_Sales)^2))
rmsetrSte
```

A baseline model is made using the mean of  the sales of each product.
```{r}
bmodel_mean <- aggregate(Item_Outlet_Sales ~ Item_Identifier, data = train4tr, mean)

bmodel_mean_predtr <- rep(0, times = dim(train4tr)[1])
for(i in 1:(dim(train4tr)[1])){
        bmodel_mean_predtr[i]<- bmodel_mean[bmodel_mean$Item_Identifier == train4tr$Item_Identifier[i],"Item_Outlet_Sales"]
        
}
rmsetr <- sqrt(mean((bmodel_mean_predtr-train4tr$Item_Outlet_Sales)^2))
rmsetr

bmodel_mean_predte <- rep(0, times = dim(train4te)[1])
for(i in 1:(dim(train4te)[1])){
        pr3 <- train4te$Item_Identifier[i]
        bmodel_mean_predte[i]<- bmodel_mean[pr3,"Item_Outlet_Sales"]
        
}
sum(is.na(bmodel_mean_predte))

for(i in 1:length(bmodel_mean_predte)){
        if(is.na(bmodel_mean_predte[i]))
        {bmodel_mean_predte[i]<- 0}
}
sum(is.na(bmodel_mean_predte))
rmsete <- sqrt(mean((bmodel_mean_predte-train4te$Item_Outlet_Sales)^2))
rmsete
```

### Linear Regression Model 

We make a linear regression model for our data. 
We firstly create a new split of data without the dummy variables. 

```{r}
comb3 <- comb3[,-c(5,8)]
Item_Outlet_Sales <- comb3$Item_Outlet_Sales
comb3 <- comb3[,-10]
comb3$Item_Outlet_Sales <- Item_Outlet_Sales

train3 <- comb3[!(is.na(comb3$Item_Outlet_Sales)),]
test3 <- comb3[is.na(comb3$Item_Outlet_Sales),]
train3$Item_Type_General <- as.factor(train3$Item_Type_General)
test3$Item_Type_General <- as.factor(test3$Item_Type_General)
train3te <- train3[-inTrain,]
train3tr <- train3[inTrain,]
```

We fit our model to all the variables except thse that are identifiers. 

```{r}
control <- trainControl(method = "cv", number = 10)

set.seed(2207)
model_lin <- train(Item_Outlet_Sales ~ . -Item_Identifier -Outlet_Identifier
                   -Outlet, data = train3tr,trControl = control,method = "lm", 
                   metric = "RMSE")
summary(model_lin)
model_lin

barplot(model_lin$finalModel$coefficients, ylim = c(-2000, 3500))
```

We exclude the Outlet and Item_Fat_Content because of some warning  we got. 

```{r}
set.seed(2207)
model_lin2 <- train(Item_Outlet_Sales ~ . -Item_Identifier -Outlet_Identifier 
                   -Item_Fat_Content -Outlet, data = train3tr,
                   trControl = control,method = "lm", metric = "RMSE")
summary(model_lin2)
model_lin2

barplot(model_lin2$finalModel$coefficients, ylim = c(-2000, 3500))
```

A variable selection process is made to choose the most apropriate variables for the model. A backwards selection process is attempted, using the AIC (Akaike Information Criterion).

```{r}

fil_l <- lm(Item_Outlet_Sales ~ . -Item_Identifier -Outlet_Identifier, 
            data = train3tr, metric = "RMSE")
bestmod <- step(fil_l, direction = "backward")

set.seed(2207)
model_lin3 <- train(Item_Outlet_Sales ~ Item_Fat_Content + Item_MRP + Outlet,
                    data = train3tr,trControl = control,method = "lm",
                    metric = "RMSE")
model_lin3

barplot(model_lin3$finalModel$coefficients, ylim = c(-2000, 3500))
```

We can see that the three linear models we tried are mostly similar, with the last one to be a little better. We use this model to predict the values of Item_Outlet_Sales of the test partition of the training data set "train3te" and find the RMSE between the predictions and the actual observations. 

```{r}
pred_lin3 <- predict(model_lin3, newdata = train3te)
RMSE(pred_lin3, train3te$Item_Outlet_Sales)
```

### Ridge and Lasso Models 

#### Ridge 

We make a model used regularised regression ridge technique. 

```{r}
set.seed(2207)
ridge <- train(Item_Outlet_Sales ~. -Item_Identifier -Outlet_Identifier,
               data = train3tr, method='ridge',lambda = 4)
ridge.pred <- predict(ridge, train3te)
RMSE(ridge.pred,train4te$Item_Outlet_Sales)
```

We tune the lambda value

```{r}
set.seed(2207)
fitControl <- trainControl(method = "cv", number = 10)
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))

set.seed(2207)
ridge2 <- train(Item_Outlet_Sales~. -Item_Identifier -Outlet_Identifier, 
               data = train3tr, method='ridge',trControl = fitControl,
               tuneGrid = lambdaGrid)
ridge2.pred <- predict(ridge2, train3te)
RMSE(ridge2.pred,train4te$Item_Outlet_Sales)
ridge2
```

We can see that the RMSE is slightly better. 

#### Lasso 

We make a model used regularised regression lasso technique.

```{r}
set.seed(2207)
Grid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001))
set.seed(2207)
lasso <- train(Item_Outlet_Sales ~. -Item_Identifier -Outlet_Identifier,
               data = train3tr, method='glmnet', trControl = fitControl, 
               tuneGrid = Grid)
lasso.pred <- predict(lasso, train3te)
RMSE(lasso.pred,train3te$Item_Outlet_Sales)
```

### Decision tree 

We try to model our data with a decision tree. 

```{r}

fitpart <- rpart(Item_Outlet_Sales ~.,data = train3tr[,-c(1,6)], method = "anova"
                 , model = TRUE)
fitpart.pred <- predict(fitpart, newdata = train3te[,-c(1,6)])
RMSE(fitpart.pred, train3te$Item_Outlet_Sales)

## Tune the model

fitpart2 <- rpart(Item_Outlet_Sales ~.,data = train3tr[,-c(1,6)], method = "anova"
                 , model = TRUE, maxdepth = 15, minsplit = 100)
fitpart.pred2 <- predict(fitpart2, newdata = train3te[,-c(1,6)])
RMSE(fitpart.pred2, train3te$Item_Outlet_Sales) 


fitpart3 <- rpart(Item_Outlet_Sales ~.,data = train3tr[,-c(1,6)], method = "anova"
                  , model = TRUE, maxdepth = 8, minsplit = 150)
fitpart.pred3 <- predict(fitpart3, newdata = train3te[,-c(1,6)])
RMSE(fitpart.pred3, train3te$Item_Outlet_Sales)
```

We tried to tune the parameter of the model, but as we can see the RMSE remains the same at every decision tree model that we made, but is still better than linear and reguralarized regression. 

### Random Forest 

We next try a random forest model to our data. 

```{r}
set.seed(2207)
rf_fit <- randomForest(Item_Outlet_Sales ~ . , data = train3tr[,-c(1,6)])
rf_fit.pred <- predict(rf_fit, newdata = train3te[,-c(1,6)])
RMSE(rf_fit.pred, train3te$Item_Outlet_Sales)
```

We changed the default number of trees to grow and the nodsize parametrs. 

```{r}
set.seed(2207)
rf_fit2 <- randomForest(Item_Outlet_Sales ~ . , data = train3tr[,-c(1,6)], 
                       ntree = 500, nodesize = 100)
rf_fit.pred2 <- predict(rf_fit2, newdata = train3te[,-c(1,6)])
RMSE(rf_fit.pred2, train3te$Item_Outlet_Sales)
```

We also used the train function of the caret package to build and tune a random forest model. However, slightly larger error than the model we built with randomForest package was achieved. Also, the caret package model needed a much greater amount of computational time in comparison with the randomForest package model and that is the reason (computational time) why the procedure of the particular model building will not be included in this report. 

## EXtreme Gradient Boosting 

We next use XGBoost alforithm to build  a model for our data. 
We use the data set that we have one-hot encoded. 

```{r}
### Xgb (with the one-hot encoded data set)

param_list <- list(objective = "reg:linear", eta = 0.01, gamma = 1, 
                   max_depth = 6, subsample = 0.8, colsample_bytree = 0.5) 

dtrain <- xgb.DMatrix(data = data.matrix(train4tr[,!(names(train4tr) %in% 
                c("Item_Identifier","Outlet_Identifier",
                "Item_Outlet_Sales"))]) ,label = train4tr$Item_Outlet_Sales)

dtest <- xgb.DMatrix(data = data.matrix(train4te[,!(names(train4te) %in%                                      c("Item_Identifier","Outlet_Identifier",
                                "Item_Outlet_Sales"))]))

## cross-validation 

set.seed(2207)
xgb.cv(params = param_list, data = dtrain, nrounds = 1000, nfold = 5, 
       print_every_n = 10, early_stopping_rounds = 30, maximize = FALSE)

## Best result in 529 

set.seed(2207)
xgb_fit <- xgb.train(data = dtrain, params = param_list, nrounds = 529)
xgb.pred <- predict(xgb_fit, newdata = dtest)
RMSE(xgb.pred, train4te$Item_Outlet_Sales)

## variance importance 

var_imp <- xgb.importance(feature_names = setdiff(names(train4tr),
                                                  c("Item_Identifier",
                                                    "Outlet_Identifier",
                                                    "Item_Outlet_Sales")), 
                          model = xgb_fit)

xgb.plot.importance(var_imp)
```

As was expected, the Item_MRP feature is the most important feature in predicting the Item_Outlet_Sales. 

## Final Training

The whole training set is trained with the most effective models, XGBoost and Random Forest. 

### XGBoost 

```{r}
### Xgb (with the one-hot encoded data set)

param_list <- list(objective = "reg:linear", eta = 0.01, gamma = 1, 
                   max_depth = 6, subsample = 0.8, colsample_bytree = 0.5) 

dtrain <- xgb.DMatrix(data = data.matrix(train4[,!(names(train4) %in% 
                c("Item_Identifier","Outlet_Identifier",
                "Item_Outlet_Sales"))]) ,label = train4$Item_Outlet_Sales)

dtest <- xgb.DMatrix(data = data.matrix(test4[,!(names(test4) %in%                                      c("Item_Identifier","Outlet_Identifier",
                                "Item_Outlet_Sales"))]))

## cross-validation 

set.seed(2207)
xgb.cv(params = param_list, data = dtrain, nrounds = 1000, nfold = 5, 
       print_every_n = 10, early_stopping_rounds = 30, maximize = FALSE)

## Best result in iteration 590 

set.seed(2207)
xgb_fit_fin <- xgb.train(data = dtrain, params = param_list, nrounds = 590)
xgb.pred_fin <- predict(xgb_fit_fin, newdata = dtest)

## variance importance 

var_imp <- xgb.importance(feature_names = setdiff(names(train4),
                                                  c("Item_Identifier",
                                                    "Outlet_Identifier",
                                                    "Item_Outlet_Sales")), 
                          model = xgb_fit_fin)

xgb.plot.importance(var_imp)
```


### Random Forest 

```{r}
set.seed(2207)
rf_fit_fin <- randomForest(Item_Outlet_Sales ~ . , data = train3[,-c(1,6)], 
                       ntree = 500, nodesize = 100)
rf_fit.pred_fin <- predict(rf_fit_fin, newdata = test3[,-c(1,6)])
```

## Conclusion 

After the analysis of the Big Mart Sales data set, and the different models we built to predict the Item_Outlet_Sales, we concluded that the most efficient model with the least Root Mean Squared Error (RMSE) was the one built with the XGBoost algorithm, followed by the model built with the Random Forest algorithm. 